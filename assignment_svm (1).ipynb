{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Question 1: What is Information Gain, and how is it used in Decision Trees?\n",
        "Answer:  \n",
        "Information Gain measures how much “information” a feature gives us about the target variable. It is based on the reduction in entropy after splitting the dataset on a feature. In Decision Trees, the algorithm chooses the feature with the highest Information Gain to make splits, ensuring that each step reduces uncertainty and improves classification accuracy"
      ],
      "metadata": {
        "id": "TA3W_qJUtoCe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Question 2: Difference between Gini Impurity and Entropy\n",
        "Answer:\n",
        "\n",
        "Entropy: Measures disorder in the dataset. It uses logarithms and can be more computationally expensive.\n",
        "\n",
        "Gini Impurity: Measures the probability of misclassification. It is simpler and faster to compute.\n",
        "\n",
        "Use cases: Entropy is more theoretical, while Gini is often preferred in practice due to efficiency."
      ],
      "metadata": {
        "id": "Y8tiKOaLtvPa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3: What is Pre-Pruning in Decision Trees?\n",
        "Answer:  \n",
        "Pre-pruning stops the tree from growing too deep by setting limits (like max depth, min samples per split). This prevents overfitting and keeps the model simpler and more generalizable."
      ],
      "metadata": {
        "id": "wBS9CNNSuAqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques 4:\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Train Decision Tree with Gini criterion\n",
        "clf = DecisionTreeClassifier(criterion='gini', random_state=42)\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Print feature importances\n",
        "print(\"Feature Importances:\", clf.feature_importances_)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEiaInd1uOQh",
        "outputId": "c8dcc9aa-4d9f-4fab-889a-adb3194b0097"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature Importances: [0.01333333 0.         0.56405596 0.42261071]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "Answer:  \n",
        "SVM is a supervised learning algorithm that finds the best hyperplane to separate classes. It maximizes the margin between data points of different classes, making it robust and effective for classification tasks."
      ],
      "metadata": {
        "id": "yJa2_ACqucfr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "Answer:  \n",
        "The Kernel Trick allows SVMs to classify data that is not linearly separable by mapping it into a higher-dimensional space. Common kernels include Linear, Polynomial, and RBF."
      ],
      "metadata": {
        "id": "nC95iakSuuLs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques-7\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "wine = load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(wine.data, wine.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Linear kernel\n",
        "svm_linear = SVC(kernel='linear')\n",
        "svm_linear.fit(X_train, y_train)\n",
        "acc_linear = accuracy_score(y_test, svm_linear.predict(X_test))\n",
        "\n",
        "# RBF kernel\n",
        "svm_rbf = SVC(kernel='rbf')\n",
        "svm_rbf.fit(X_train, y_train)\n",
        "acc_rbf = accuracy_score(y_test, svm_rbf.predict(X_test))\n",
        "\n",
        "print(\"Linear Kernel Accuracy:\", acc_linear)\n",
        "print(\"RBF Kernel Accuracy:\", acc_rbf)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmJtuCGzu4tL",
        "outputId": "93614e00-30ce-4911-9b60-ec1b183f87f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Kernel Accuracy: 0.9814814814814815\n",
            "RBF Kernel Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "Question 8: What is Naïve Bayes, and why is it called \"Naïve\"?\n",
        "Answer:  \n",
        "Naïve Bayes is a probabilistic classifier based on Bayes’ theorem. It assumes that features are independent of each other (the “naïve” assumption), which is rarely true in reality but works surprisingly well in practice."
      ],
      "metadata": {
        "id": "280SB0Q3vHqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Types of Naïve Bayes\n",
        "Answer:\n",
        "\n",
        "Gaussian NB: Assumes features follow a normal distribution.\n",
        "\n",
        "Multinomial NB: Best for discrete counts (e.g., word frequencies in text).\n",
        "\n",
        "Bernoulli NB: Works with binary features (yes/no, present/absent)."
      ],
      "metadata": {
        "id": "4-2BsBkNvn5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#ques10:\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
        "\n",
        "# Train Gaussian NB\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld3F5k-EvqkN",
        "outputId": "29816144-bba1-4037-ad11-acb1c262f015"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9415204678362573\n"
          ]
        }
      ]
    }
  ]
}